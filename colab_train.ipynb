{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis TrackNet — Colab Training\n",
    "\n",
    "Train TrackNet on the tennis dataset using **pretrained shuttlecock (badminton) weights** from the original TrackNetV3 repo.\n",
    "\n",
    "**What this does:**\n",
    "- Clones your private repo\n",
    "- Downloads the original TrackNet v1 tennis dataset\n",
    "- Downloads pretrained badminton weights\n",
    "- Fine-tunes on tennis with those weights as initialization\n",
    "\n",
    "**Requirements:** GPU runtime (T4 or better), Google Drive mounted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU\n!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n\nimport torch\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for persistent storage across sessions)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Persistent storage dir\n",
    "DRIVE_DIR = '/content/drive/MyDrive/tennis-tracknet'\n",
    "!mkdir -p {DRIVE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone private repo (will prompt for GitHub auth)\n",
    "import os\n",
    "if not os.path.exists('/content/tennis-tracknet'):\n",
    "    !git clone https://github.com/smyng/tennis-tracknet.git /content/tennis-tracknet\n",
    "else:\n",
    "    !cd /content/tennis-tracknet && git pull\n",
    "\n",
    "os.chdir('/content/tennis-tracknet')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q parse tqdm tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Downloads the TrackNet v1 tennis dataset and converts it to TrackNetV3 format.\n",
    "Converted data is cached in Google Drive so you only do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DRIVE = f'{DRIVE_DIR}/data'\n",
    "DATA_LOCAL = '/content/tennis-tracknet/data'\n",
    "\n",
    "# Check if converted data already exists in Drive\n",
    "if os.path.exists(f'{DATA_DRIVE}/train/match1'):\n",
    "    print('Converted dataset found in Drive, symlinking...')\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    !ls {DATA_LOCAL}/train/\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('No converted dataset in Drive. Will download and convert.')\n",
    "    print('This takes ~15-20 min the first time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download raw tennis dataset (skip if data already linked above)\nimport os\nif not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n    !pip install -q gdown\n    \n    RAW_DIR = '/content/raw-tennis-dataset'\n    !mkdir -p {RAW_DIR}\n    \n    # Download from the TrackNet v1 dataset Google Drive\n    # Folder: https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut\n    import gdown\n    gdown.download_folder(\n        'https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut',\n        output=RAW_DIR, quiet=False\n    )\n    \n    # The dataset is inside Dataset.zip — unzip it\n    import zipfile\n    zip_path = os.path.join(RAW_DIR, 'Dataset.zip')\n    if os.path.exists(zip_path):\n        print('Extracting Dataset.zip...')\n        with zipfile.ZipFile(zip_path, 'r') as z:\n            z.extractall(RAW_DIR)\n        os.remove(zip_path)\n    \n    # Find the directory containing game1/, game2/, etc.\n    DATASET_DIR = RAW_DIR\n    for candidate in [os.path.join(RAW_DIR, 'Dataset'), RAW_DIR]:\n        if os.path.exists(os.path.join(candidate, 'game1')):\n            DATASET_DIR = candidate\n            break\n    \n    print(f'Dataset directory: {DATASET_DIR}')\n    !ls {DATASET_DIR}/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert dataset and save to Drive for persistence\nimport os\nif not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n    # Find the dataset directory (set by previous cell, or detect it)\n    if 'DATASET_DIR' not in dir():\n        RAW_DIR = '/content/raw-tennis-dataset'\n        DATASET_DIR = os.path.join(RAW_DIR, 'Dataset') if os.path.exists(os.path.join(RAW_DIR, 'Dataset', 'game1')) else RAW_DIR\n\n    !python scripts/convert_tennis_dataset.py \\\n        --input {DATASET_DIR} \\\n        --output {DATA_DRIVE} \\\n        --test-games 9 10 \\\n        --verbose\n    \n    # Symlink Drive data into repo\n    !rm -rf {DATA_LOCAL}\n    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n    \n    print('Conversion complete.')\n    !ls {DATA_LOCAL}/train/"
  },
  {
   "cell_type": "code",
   "source": "# Generate median.npz files (required by dataset.py for background subtraction)\n# Saved to Drive, so this only runs once\nimport os, cv2, numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndata_dir = DATA_DRIVE\nneeds_generation = False\n\n# Check if median files already exist\nfor split in ['train', 'test']:\n    split_dir = Path(data_dir) / split\n    if not split_dir.exists():\n        continue\n    for match_dir in split_dir.iterdir():\n        frame_root = match_dir / 'frame'\n        if not frame_root.exists():\n            continue\n        for rally_dir in frame_root.iterdir():\n            if rally_dir.is_dir() and not (rally_dir / 'median.npz').exists():\n                needs_generation = True\n                break\n        if needs_generation:\n            break\n    if needs_generation:\n        break\n\nif needs_generation:\n    print('Generating median.npz files (one-time)...')\n    for split in ['train', 'test']:\n        split_dir = Path(data_dir) / split\n        if not split_dir.exists():\n            continue\n        for match_dir in sorted(split_dir.iterdir()):\n            if not match_dir.is_dir():\n                continue\n            frame_root = match_dir / 'frame'\n            if not frame_root.exists():\n                continue\n            rally_medians = []\n            for rally_dir in tqdm(sorted(frame_root.iterdir()), desc=f'{split}/{match_dir.name}'):\n                if not rally_dir.is_dir():\n                    continue\n                median_file = rally_dir / 'median.npz'\n                if median_file.exists():\n                    rally_medians.append(np.load(str(median_file))['median'])\n                    continue\n                frames = sorted(rally_dir.glob('*.png'))\n                if not frames:\n                    continue\n                step = max(1, len(frames) // 50)\n                sampled = frames[::step][:50]\n                imgs = [cv2.imread(str(f))[..., ::-1] for f in sampled]\n                median = np.median(np.array(imgs), axis=0)\n                np.savez(str(median_file), median=median)\n                rally_medians.append(median)\n            # Match-level median\n            match_median = match_dir / 'median.npz'\n            if not match_median.exists() and rally_medians:\n                median = np.median(np.array(rally_medians), axis=0)\n                np.savez(str(match_median), median=median)\n    print('Done.')\nelse:\n    print('median.npz files already exist.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrained Weights\n",
    "\n",
    "Download the original TrackNetV3 badminton (shuttlecock) checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nCKPT_DIR = '/content/tennis-tracknet/ckpts'\nos.makedirs(CKPT_DIR, exist_ok=True)\n\nif not os.path.exists(f'{CKPT_DIR}/TrackNet_best.pt'):\n    !pip install -q gdown\n    import gdown\n    \n    # Original TrackNetV3 checkpoints\n    # https://drive.google.com/file/d/1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA/view\n    gdown.download(\n        'https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA',\n        output='/content/TrackNetV3_ckpts.zip', quiet=False\n    )\n    !cd /content && unzip -o TrackNetV3_ckpts.zip -d /content/ckpts_tmp/\n    !rm /content/TrackNetV3_ckpts.zip\n    \n    # The zip contains a nested ckpts/ folder — flatten it\n    import glob, shutil\n    for pt_file in glob.glob('/content/ckpts_tmp/**/*.pt', recursive=True):\n        shutil.move(pt_file, CKPT_DIR)\n    !rm -rf /content/ckpts_tmp\n\nprint('Pretrained checkpoints:')\n!ls -lh {CKPT_DIR}/*.pt\n\n# Verify checkpoint\nimport torch\nckpt = torch.load(f'{CKPT_DIR}/TrackNet_best.pt', map_location='cpu', weights_only=False)\nprint(f\"\\nPretrained model: epoch {ckpt['epoch']}, bg_mode='{ckpt['param_dict']['bg_mode']}'\")\nprint(f\"Original training: seq_len={ckpt['param_dict']['seq_len']}, batch_size={ckpt['param_dict']['batch_size']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train\n",
    "\n",
    "Fine-tune TrackNet on tennis using the pretrained shuttlecock weights.\n",
    "\n",
    "**Note:** Uses `bg_mode='concat'` to match the pretrained model's architecture (27 input channels).\n",
    "Your local run uses `subtract_concat` (32 channels) which can't directly load these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "EXP_NAME = 'tennis_pretrained_shuttlecock'\n",
    "SAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'  # Save to Drive for persistence\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Also symlink exps into repo so train.py can find them\n",
    "!mkdir -p {DRIVE_DIR}/exps\n",
    "!rm -rf /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "print(f'Experiment: {EXP_NAME}')\n",
    "print(f'Checkpoints will be saved to: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with pretrained shuttlecock weights\n",
    "!python train.py \\\n",
    "    --model_name TrackNet \\\n",
    "    --seq_len 8 \\\n",
    "    --epochs 30 \\\n",
    "    --batch_size 10 \\\n",
    "    --optim Adam \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --bg_mode concat \\\n",
    "    --alpha 0.5 \\\n",
    "    --tolerance 4 \\\n",
    "    --save_dir exps/{EXP_NAME} \\\n",
    "    --pretrained ckpts/TrackNet_best.pt \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resume Training (if disconnected)\n",
    "\n",
    "Run this if your session disconnected. Checkpoints are saved to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training (run Setup cells 1-3 first, then this)\n",
    "EXP_NAME = 'tennis_pretrained_shuttlecock'\n",
    "\n",
    "# Re-link Drive dirs\n",
    "!rm -rf /content/tennis-tracknet/data /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/data /content/tennis-tracknet/data\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "!python train.py \\\n",
    "    --model_name TrackNet \\\n",
    "    --epochs 30 \\\n",
    "    --save_dir exps/{EXP_NAME} \\\n",
    "    --resume_training \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitor & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training progress\n",
    "import torch, os, glob\n",
    "\n",
    "EXP_NAME = 'tennis_pretrained_shuttlecock'\n",
    "save_dir = f'{DRIVE_DIR}/exps/{EXP_NAME}'\n",
    "\n",
    "for name in ['TrackNet_cur.pt', 'TrackNet_best.pt']:\n",
    "    path = os.path.join(save_dir, name)\n",
    "    if os.path.exists(path):\n",
    "        ckpt = torch.load(path, map_location='cpu', weights_only=False)\n",
    "        print(f'{name}:')\n",
    "        print(f\"  Epoch: {ckpt['epoch'] + 1} / 30\")\n",
    "        print(f\"  Best val accuracy: {ckpt['max_val_acc']:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {DRIVE_DIR}/exps/{EXP_NAME}/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "EXP_NAME = 'tennis_pretrained_shuttlecock'\n",
    "\n",
    "!python test.py \\\n",
    "    --split test \\\n",
    "    --tracknet_file exps/{EXP_NAME}/TrackNet_best.pt \\\n",
    "    --save_dir exps/{EXP_NAME}/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best model back to Drive for download\n",
    "EXP_NAME = 'tennis_pretrained_shuttlecock'\n",
    "!cp exps/{EXP_NAME}/TrackNet_best.pt {DRIVE_DIR}/{EXP_NAME}_best.pt\n",
    "print(f'Best model saved to: {DRIVE_DIR}/{EXP_NAME}_best.pt')\n",
    "print('You can download it from Google Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}