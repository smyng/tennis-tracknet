{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis TrackNet 2x — Colab Training\n",
    "\n",
    "Train **TrackNet2x** (576x1024 resolution) on the tennis dataset with GPU optimizations.\n",
    "\n",
    "**Optimizations enabled on Colab:**\n",
    "- Mixed precision (FP16) — ~2x speedup\n",
    "- torch.compile — ~1.3x speedup\n",
    "- Larger batch size (8-16 vs 2-4 locally)\n",
    "- Precomputed frames for fast data loading\n",
    "\n",
    "**Requirements:** GPU runtime (T4 or better), Google Drive mounted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-80GB, 81920 MiB\n",
      "PyTorch: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Memory: 85.1 GB\n",
      "Recommended batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f'Memory: {mem_gb:.1f} GB')\n",
    "    # Recommend batch size based on GPU memory\n",
    "    if mem_gb >= 40:\n",
    "        print('Recommended batch_size: 16')\n",
    "    elif mem_gb >= 15:\n",
    "        print('Recommended batch_size: 8')\n",
    "    else:\n",
    "        print('Recommended batch_size: 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (for persistent storage across sessions)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Persistent storage dir\n",
    "DRIVE_DIR = '/content/drive/MyDrive/tennis-tracknet'\n",
    "!mkdir -p {DRIVE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (3/3), 526 bytes | 526.00 KiB/s, done.\n",
      "From https://github.com/smyng/tennis-tracknet\n",
      "   007a6b8..ce34b22  main       -> origin/main\n",
      "Updating 007a6b8..ce34b22\n",
      "Fast-forward\n",
      " train.py | 11 \u001b[32m++++++\u001b[m\u001b[31m-----\u001b[m\n",
      " 1 file changed, 6 insertions(+), 5 deletions(-)\n",
      "/content/tennis-tracknet\n"
     ]
    }
   ],
   "source": [
    "# Clone private repo (will prompt for GitHub auth)\n",
    "import os\n",
    "if not os.path.exists('/content/tennis-tracknet'):\n",
    "    !git clone https://github.com/smyng/tennis-tracknet.git /content/tennis-tracknet\n",
    "else:\n",
    "    !cd /content/tennis-tracknet && git pull\n",
    "\n",
    "os.chdir('/content/tennis-tracknet')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q parse tqdm tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Downloads the TrackNet v1 tennis dataset and converts it to TrackNetV3 format.\n",
    "Converted data is cached in Google Drive so you only do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted dataset found in Drive, symlinking...\n",
      "match1\tmatch2\tmatch3\tmatch4\tmatch5\tmatch6\tmatch7\tmatch8\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DRIVE = f'{DRIVE_DIR}/data'\n",
    "DATA_LOCAL = '/content/tennis-tracknet/data'\n",
    "\n",
    "# Check if converted data already exists in Drive\n",
    "if os.path.exists(f'{DATA_DRIVE}/train/match1'):\n",
    "    print('Converted dataset found in Drive, symlinking...')\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    !ls {DATA_LOCAL}/train/\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('No converted dataset in Drive. Will download and convert.')\n",
    "    print('This takes ~15-20 min the first time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download raw tennis dataset (skip if data already linked above)\n",
    "import os\n",
    "if not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    RAW_DIR = '/content/raw-tennis-dataset'\n",
    "    !mkdir -p {RAW_DIR}\n",
    "    \n",
    "    # Download from the TrackNet v1 dataset Google Drive\n",
    "    # Folder: https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut\n",
    "    import gdown\n",
    "    gdown.download_folder(\n",
    "        'https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut',\n",
    "        output=RAW_DIR, quiet=False\n",
    "    )\n",
    "    \n",
    "    # The dataset is inside Dataset.zip — unzip it\n",
    "    import zipfile\n",
    "    zip_path = os.path.join(RAW_DIR, 'Dataset.zip')\n",
    "    if os.path.exists(zip_path):\n",
    "        print('Extracting Dataset.zip...')\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(RAW_DIR)\n",
    "        os.remove(zip_path)\n",
    "    \n",
    "    # Find the directory containing game1/, game2/, etc.\n",
    "    DATASET_DIR = RAW_DIR\n",
    "    for candidate in [os.path.join(RAW_DIR, 'Dataset'), RAW_DIR]:\n",
    "        if os.path.exists(os.path.join(candidate, 'game1')):\n",
    "            DATASET_DIR = candidate\n",
    "            break\n",
    "    \n",
    "    print(f'Dataset directory: {DATASET_DIR}')\n",
    "    !ls {DATASET_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset and save to Drive for persistence\n",
    "import os\n",
    "if not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n",
    "    # Find the dataset directory (set by previous cell, or detect it)\n",
    "    if 'DATASET_DIR' not in dir():\n",
    "        RAW_DIR = '/content/raw-tennis-dataset'\n",
    "        DATASET_DIR = os.path.join(RAW_DIR, 'Dataset') if os.path.exists(os.path.join(RAW_DIR, 'Dataset', 'game1')) else RAW_DIR\n",
    "\n",
    "    !python scripts/convert_tennis_dataset.py \\\n",
    "        --input {DATASET_DIR} \\\n",
    "        --output {DATA_DRIVE} \\\n",
    "        --test-games 9 10 \\\n",
    "        --verbose\n",
    "    \n",
    "    # Symlink Drive data into repo\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    \n",
    "    print('Conversion complete.')\n",
    "    !ls {DATA_LOCAL}/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median.npz files already exist.\n"
     ]
    }
   ],
   "source": [
    "# Generate median.npz files (required by dataset.py for background subtraction)\n",
    "# Saved to Drive, so this only runs once\n",
    "import os, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = DATA_DRIVE\n",
    "needs_generation = False\n",
    "\n",
    "# Check if median files already exist\n",
    "for split in ['train', 'test']:\n",
    "    split_dir = Path(data_dir) / split\n",
    "    if not split_dir.exists():\n",
    "        continue\n",
    "    for match_dir in split_dir.iterdir():\n",
    "        frame_root = match_dir / 'frame'\n",
    "        if not frame_root.exists():\n",
    "            continue\n",
    "        for rally_dir in frame_root.iterdir():\n",
    "            if rally_dir.is_dir() and not (rally_dir / 'median.npz').exists():\n",
    "                needs_generation = True\n",
    "                break\n",
    "        if needs_generation:\n",
    "            break\n",
    "    if needs_generation:\n",
    "        break\n",
    "\n",
    "if needs_generation:\n",
    "    print('Generating median.npz files (one-time)...')\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = Path(data_dir) / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        for match_dir in sorted(split_dir.iterdir()):\n",
    "            if not match_dir.is_dir():\n",
    "                continue\n",
    "            frame_root = match_dir / 'frame'\n",
    "            if not frame_root.exists():\n",
    "                continue\n",
    "            rally_medians = []\n",
    "            for rally_dir in tqdm(sorted(frame_root.iterdir()), desc=f'{split}/{match_dir.name}'):\n",
    "                if not rally_dir.is_dir():\n",
    "                    continue\n",
    "                median_file = rally_dir / 'median.npz'\n",
    "                if median_file.exists():\n",
    "                    rally_medians.append(np.load(str(median_file))['median'])\n",
    "                    continue\n",
    "                frames = sorted(rally_dir.glob('*.png'))\n",
    "                if not frames:\n",
    "                    continue\n",
    "                step = max(1, len(frames) // 50)\n",
    "                sampled = frames[::step][:50]\n",
    "                imgs = [cv2.imread(str(f))[..., ::-1] for f in sampled]\n",
    "                median = np.median(np.array(imgs), axis=0)\n",
    "                np.savez(str(median_file), median=median)\n",
    "                rally_medians.append(median)\n",
    "            # Match-level median\n",
    "            match_median = match_dir / 'median.npz'\n",
    "            if not match_median.exists() and rally_medians:\n",
    "                median = np.median(np.array(rally_medians), axis=0)\n",
    "                np.savez(str(match_median), median=median)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('median.npz files already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrained Weights\n",
    "\n",
    "Download the original TrackNetV3 badminton (shuttlecock) checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA\n",
      "From (redirected): https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA&confirm=t&uuid=5dda465a-d001-4285-8abf-2e29c67482c2\n",
      "To: /content/TrackNetV3_ckpts.zip\n",
      "100%|██████████| 132M/132M [00:00<00:00, 226MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  TrackNetV3_ckpts.zip\n",
      "   creating: /content/ckpts_tmp/ckpts/\n",
      "  inflating: /content/ckpts_tmp/ckpts/InpaintNet_best.pt  \n",
      "  inflating: /content/ckpts_tmp/ckpts/TrackNet_best.pt  \n",
      "Pretrained checkpoints:\n",
      "-rw-r--r-- 1 root root 6.0M Aug  8  2023 /content/tennis-tracknet/ckpts/InpaintNet_best.pt\n",
      "-rw-r--r-- 1 root root 130M Aug  8  2023 /content/tennis-tracknet/ckpts/TrackNet_best.pt\n",
      "\n",
      "Pretrained model: epoch 18, bg_mode='concat'\n",
      "Original training: seq_len=8, batch_size=10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "CKPT_DIR = '/content/tennis-tracknet/ckpts'\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(f'{CKPT_DIR}/TrackNet_best.pt'):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    \n",
    "    # Original TrackNetV3 checkpoints\n",
    "    # https://drive.google.com/file/d/1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA/view\n",
    "    gdown.download(\n",
    "        'https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA',\n",
    "        output='/content/TrackNetV3_ckpts.zip', quiet=False\n",
    "    )\n",
    "    !cd /content && unzip -o TrackNetV3_ckpts.zip -d /content/ckpts_tmp/\n",
    "    !rm /content/TrackNetV3_ckpts.zip\n",
    "    \n",
    "    # The zip contains a nested ckpts/ folder — flatten it\n",
    "    import glob, shutil\n",
    "    for pt_file in glob.glob('/content/ckpts_tmp/**/*.pt', recursive=True):\n",
    "        shutil.move(pt_file, CKPT_DIR)\n",
    "    !rm -rf /content/ckpts_tmp\n",
    "\n",
    "print('Pretrained checkpoints:')\n",
    "!ls -lh {CKPT_DIR}/*.pt\n",
    "\n",
    "# Verify checkpoint\n",
    "import torch\n",
    "ckpt = torch.load(f'{CKPT_DIR}/TrackNet_best.pt', map_location='cpu', weights_only=False)\n",
    "print(f\"\\nPretrained model: epoch {ckpt['epoch']}, bg_mode='{ckpt['param_dict']['bg_mode']}'\")\n",
    "print(f\"Original training: seq_len={ckpt['param_dict']['seq_len']}, batch_size={ckpt['param_dict']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Precompute Frames (one-time)\n",
    "\n",
    "Precomputes resized + background-subtracted frames as `.npy` files for fast data loading.\n",
    "Cached in Google Drive so this only runs once (~20 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing frames (62 found, need ~95)...\n",
      "Output directory: /content/tennis-tracknet/data/precomputed/subtract_concat_576x1024\n",
      "\n",
      "train: 74 rallies\n",
      "  train: 100% 74/74 [01:25<00:00,  1.15s/it]\n",
      "\n",
      "val: 21 rallies\n",
      "  val: 100% 21/21 [02:37<00:00,  7.48s/it]\n",
      "\n",
      "Done: 95 files, 43.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Precompute frames (skip if already done)\n",
    "# Since data/ is symlinked to Drive, output goes to Drive automatically\n",
    "# Uses parallel workers + threaded I/O for ~3-4x speedup on Drive\n",
    "import os, glob\n",
    "\n",
    "PRECOMPUTE_DIR = os.path.join(DATA_LOCAL, 'precomputed', 'subtract_concat_576x1024')\n",
    "\n",
    "existing = glob.glob(os.path.join(PRECOMPUTE_DIR, '*.npy'))\n",
    "if len(existing) > 90:\n",
    "    print(f'Precomputed frames found: {len(existing)} files — skipping')\n",
    "else:\n",
    "    print(f'Precomputing frames ({len(existing)} found, need ~95)...')\n",
    "    !python precompute_frames.py \\\n",
    "        --data_dir {DATA_LOCAL} \\\n",
    "        --bg_mode subtract_concat \\\n",
    "        --height 576 --width 1024 \\\n",
    "        --workers 4 --io_threads 8 \\\n",
    "        --splits train val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy precomputed frames to local SSD for ~10x faster data loading\n",
    "# Google Drive FUSE is slow for large file reads — local SSD eliminates the I/O bottleneck\n",
    "# This takes ~5-10 min but saves significant time per epoch\n",
    "import os, time\n",
    "\n",
    "PRECOMPUTE_LOCAL = '/content/precomputed_local'\n",
    "\n",
    "if not os.path.exists(PRECOMPUTE_LOCAL):\n",
    "    print('Copying precomputed frames from Drive to local SSD...')\n",
    "    start = time.time()\n",
    "    !cp -r {DATA_DRIVE}/precomputed {PRECOMPUTE_LOCAL}\n",
    "    elapsed = time.time() - start\n",
    "    print(f'Done in {elapsed/60:.1f} min')\n",
    "    !du -sh {PRECOMPUTE_LOCAL}\n",
    "else:\n",
    "    print('Local precomputed copy already exists')\n",
    "    !du -sh {PRECOMPUTE_LOCAL}\n",
    "\n",
    "# Restructure data/ to read precomputed from local SSD, everything else from Drive\n",
    "# Remove the flat symlink and create a hybrid directory\n",
    "!rm -f {DATA_LOCAL}\n",
    "!mkdir -p {DATA_LOCAL}\n",
    "\n",
    "# Symlink all Drive contents except precomputed\n",
    "for item in os.listdir(DATA_DRIVE):\n",
    "    if item == 'precomputed':\n",
    "        continue\n",
    "    src = os.path.join(DATA_DRIVE, item)\n",
    "    dst = os.path.join(DATA_LOCAL, item)\n",
    "    if not os.path.exists(dst):\n",
    "        os.symlink(src, dst)\n",
    "\n",
    "# Point precomputed to local SSD copy\n",
    "precomputed_dst = os.path.join(DATA_LOCAL, 'precomputed')\n",
    "if not os.path.exists(precomputed_dst):\n",
    "    os.symlink(PRECOMPUTE_LOCAL, precomputed_dst)\n",
    "\n",
    "print(f'data/ structure:')\n",
    "!ls -la {DATA_LOCAL}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint: epoch 3, best_val_acc=0.3011\n"
     ]
    }
   ],
   "source": [
    "# Upload local checkpoint to resume training on Colab\n",
    "# Run this cell, then select your TrackNet2x_cur.pt file from local machine\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "SAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink exps into repo\n",
    "!mkdir -p {DRIVE_DIR}/exps\n",
    "!rm -rf /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "# Upload checkpoint\n",
    "# print('Upload TrackNet2x_cur.pt (and optionally TrackNet2x_best.pt):')\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for name, data in uploaded.items():\n",
    "#     dest = os.path.join(SAVE_DIR, name)\n",
    "#     with open(dest, 'wb') as f:\n",
    "#         f.write(data)\n",
    "#     print(f'Saved {name} to {dest} ({len(data)/1e6:.1f} MB)')\n",
    "\n",
    "# Verify\n",
    "import torch\n",
    "ckpt = torch.load(os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt'), map_location='cpu', weights_only=False)\n",
    "print(f\"\\nCheckpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train\n",
    "\n",
    "**Option A:** Start fresh training from scratch (cell below).\n",
    "**Option B:** Resume from a local checkpoint — upload it first, then use the resume cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: tennis_2x_colab\n",
      "Batch size: 16 (GPU: 85 GB)\n",
      "Checkpoints: /content/drive/MyDrive/tennis-tracknet/exps/tennis_2x_colab\n",
      "Found checkpoint: epoch 3, best_val_acc=0.3011\n",
      "Use the RESUME cell below to continue training.\n"
     ]
    }
   ],
   "source": [
    "# Training config (used by both fresh start and resume)\n",
    "import os, torch\n",
    "\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "SAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink exps into repo so train.py can find them\n",
    "!mkdir -p {DRIVE_DIR}/exps\n",
    "!rm -rf /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "# Batch size: 16 for A100, 8 for T4/V100, 4 for P100\n",
    "mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "BATCH_SIZE = 16 if mem_gb >= 40 else (8 if mem_gb >= 15 else 4)\n",
    "\n",
    "print(f'Experiment: {EXP_NAME}')\n",
    "print(f'Batch size: {BATCH_SIZE} (GPU: {mem_gb:.0f} GB)')\n",
    "print(f'Checkpoints: {SAVE_DIR}')\n",
    "\n",
    "# Check for existing checkpoint\n",
    "ckpt_path = os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt')\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    print(f\"Found checkpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")\n",
    "    print('Use the RESUME cell below to continue training.')\n",
    "else:\n",
    "    print('No checkpoint found. Use the FRESH START cell below.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-15 02:39:21.630189: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-15 02:39:21.650695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771123161.674887   15331 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771123161.682830   15331 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771123161.703581   15331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771123161.703617   15331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771123161.703620   15331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771123161.703623   15331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-15 02:39:21.708931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "TensorBoard: start with 'tensorboard --logdir exps/tennis_2x_colab/logs', view at http://localhost:6006/\n",
      "Load checkpoint from TrackNet2x_cur.pt...\n",
      "Parameters: {'model_name': 'TrackNet2x', 'seq_len': 8, 'epochs': 30, 'batch_size': 16, 'optim': 'Adam', 'learning_rate': 0.001, 'lr_scheduler': '', 'bg_mode': 'subtract_concat', 'alpha': -1, 'frame_alpha': 0.5, 'mask_ratio': 0.3, 'tolerance': 4, 'resume_training': True, 'seed': 13, 'save_dir': 'exps/tennis_2x_colab', 'pretrained': '', 'height': 576, 'width': 1024, 'debug': False, 'verbose': True, 'fp16': True, 'compile': True, 'num_workers': 4}\n",
      "Load dataset...\n",
      "Using device: cuda\n",
      "Create TrackNet2x...\n",
      "Mixed precision (FP16) enabled\n",
      "Resume training from epoch 3...\n",
      "Compiling model with torch.compile...\n",
      "Start training...\n",
      "Epoch [4 / 30]\n",
      "Training:  64% 628/975 [1:38:02<35:37,  6.16s/it, loss=2.15e-5]  "
     ]
    }
   ],
   "source": [
    "# RESUME — continue training from checkpoint (local or previous Colab run)\n",
    "!python train.py \\\n",
    "    --model_name TrackNet2x \\\n",
    "    --epochs 30 \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --fp16 \\\n",
    "    --compile \\\n",
    "    --num_workers 4 \\\n",
    "    --save_dir exps/{EXP_NAME} \\\n",
    "    --resume_training \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {DRIVE_DIR}/exps/{EXP_NAME}/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "\n",
    "!python test.py \\\n",
    "    --split test \\\n",
    "    --tracknet_file exps/{EXP_NAME}/TrackNet2x_best.pt \\\n",
    "    --save_dir exps/{EXP_NAME}/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best model back to Drive for download\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "!cp exps/{EXP_NAME}/TrackNet2x_best.pt {DRIVE_DIR}/{EXP_NAME}_best.pt\n",
    "print(f'Best model saved to: {DRIVE_DIR}/{EXP_NAME}_best.pt')\n",
    "print('You can download it from Google Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}