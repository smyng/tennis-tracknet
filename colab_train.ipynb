{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis TrackNet 2x — Colab Training\n",
    "\n",
    "Train **TrackNet2x** (576x1024 resolution) on the tennis dataset with GPU optimizations.\n",
    "\n",
    "**Optimizations enabled on Colab:**\n",
    "- Mixed precision (FP16) — ~2x speedup\n",
    "- torch.compile — ~1.3x speedup\n",
    "- Larger batch size (8-16 vs 2-4 locally)\n",
    "- Precomputed frames for fast data loading\n",
    "\n",
    "**Requirements:** GPU runtime (T4 or better), Google Drive mounted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-80GB, 81920 MiB\n",
      "PyTorch: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Memory: 85.1 GB\n",
      "Recommended batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f'Memory: {mem_gb:.1f} GB')\n",
    "    # Recommend batch size based on GPU memory\n",
    "    if mem_gb >= 40:\n",
    "        print('Recommended batch_size: 16')\n",
    "    elif mem_gb >= 15:\n",
    "        print('Recommended batch_size: 8')\n",
    "    else:\n",
    "        print('Recommended batch_size: 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (for persistent storage across sessions)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Persistent storage dir\n",
    "DRIVE_DIR = '/content/drive/MyDrive/tennis-tracknet'\n",
    "!mkdir -p {DRIVE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/tennis-tracknet'...\n",
      "remote: Enumerating objects: 57, done.\u001b[K\n",
      "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
      "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
      "remote: Total 57 (delta 26), reused 46 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (57/57), 69.81 KiB | 11.64 MiB/s, done.\n",
      "Resolving deltas: 100% (26/26), done.\n",
      "/content/tennis-tracknet\n"
     ]
    }
   ],
   "source": [
    "# Clone private repo (will prompt for GitHub auth)\n",
    "import os\n",
    "if not os.path.exists('/content/tennis-tracknet'):\n",
    "    !git clone https://github.com/smyng/tennis-tracknet.git /content/tennis-tracknet\n",
    "else:\n",
    "    !cd /content/tennis-tracknet && git pull\n",
    "\n",
    "os.chdir('/content/tennis-tracknet')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q parse tqdm tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Downloads the TrackNet v1 tennis dataset and converts it to TrackNetV3 format.\n",
    "Converted data is cached in Google Drive so you only do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted dataset found in Drive, symlinking...\n",
      "match1\tmatch2\tmatch3\tmatch4\tmatch5\tmatch6\tmatch7\tmatch8\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DRIVE = f'{DRIVE_DIR}/data'\n",
    "DATA_LOCAL = '/content/tennis-tracknet/data'\n",
    "\n",
    "# Check if converted data already exists in Drive\n",
    "if os.path.exists(f'{DATA_DRIVE}/train/match1'):\n",
    "    print('Converted dataset found in Drive, symlinking...')\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    !ls {DATA_LOCAL}/train/\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('No converted dataset in Drive. Will download and convert.')\n",
    "    print('This takes ~15-20 min the first time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download raw tennis dataset (skip if data already linked above)\n",
    "import os\n",
    "if not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    RAW_DIR = '/content/raw-tennis-dataset'\n",
    "    !mkdir -p {RAW_DIR}\n",
    "    \n",
    "    # Download from the TrackNet v1 dataset Google Drive\n",
    "    # Folder: https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut\n",
    "    import gdown\n",
    "    gdown.download_folder(\n",
    "        'https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut',\n",
    "        output=RAW_DIR, quiet=False\n",
    "    )\n",
    "    \n",
    "    # The dataset is inside Dataset.zip — unzip it\n",
    "    import zipfile\n",
    "    zip_path = os.path.join(RAW_DIR, 'Dataset.zip')\n",
    "    if os.path.exists(zip_path):\n",
    "        print('Extracting Dataset.zip...')\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(RAW_DIR)\n",
    "        os.remove(zip_path)\n",
    "    \n",
    "    # Find the directory containing game1/, game2/, etc.\n",
    "    DATASET_DIR = RAW_DIR\n",
    "    for candidate in [os.path.join(RAW_DIR, 'Dataset'), RAW_DIR]:\n",
    "        if os.path.exists(os.path.join(candidate, 'game1')):\n",
    "            DATASET_DIR = candidate\n",
    "            break\n",
    "    \n",
    "    print(f'Dataset directory: {DATASET_DIR}')\n",
    "    !ls {DATASET_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset and save to Drive for persistence\n",
    "import os\n",
    "if not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n",
    "    # Find the dataset directory (set by previous cell, or detect it)\n",
    "    if 'DATASET_DIR' not in dir():\n",
    "        RAW_DIR = '/content/raw-tennis-dataset'\n",
    "        DATASET_DIR = os.path.join(RAW_DIR, 'Dataset') if os.path.exists(os.path.join(RAW_DIR, 'Dataset', 'game1')) else RAW_DIR\n",
    "\n",
    "    !python scripts/convert_tennis_dataset.py \\\n",
    "        --input {DATASET_DIR} \\\n",
    "        --output {DATA_DRIVE} \\\n",
    "        --test-games 9 10 \\\n",
    "        --verbose\n",
    "    \n",
    "    # Symlink Drive data into repo\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    \n",
    "    print('Conversion complete.')\n",
    "    !ls {DATA_LOCAL}/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median.npz files already exist.\n"
     ]
    }
   ],
   "source": [
    "# Generate median.npz files (required by dataset.py for background subtraction)\n",
    "# Saved to Drive, so this only runs once\n",
    "import os, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = DATA_DRIVE\n",
    "needs_generation = False\n",
    "\n",
    "# Check if median files already exist\n",
    "for split in ['train', 'test']:\n",
    "    split_dir = Path(data_dir) / split\n",
    "    if not split_dir.exists():\n",
    "        continue\n",
    "    for match_dir in split_dir.iterdir():\n",
    "        frame_root = match_dir / 'frame'\n",
    "        if not frame_root.exists():\n",
    "            continue\n",
    "        for rally_dir in frame_root.iterdir():\n",
    "            if rally_dir.is_dir() and not (rally_dir / 'median.npz').exists():\n",
    "                needs_generation = True\n",
    "                break\n",
    "        if needs_generation:\n",
    "            break\n",
    "    if needs_generation:\n",
    "        break\n",
    "\n",
    "if needs_generation:\n",
    "    print('Generating median.npz files (one-time)...')\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = Path(data_dir) / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        for match_dir in sorted(split_dir.iterdir()):\n",
    "            if not match_dir.is_dir():\n",
    "                continue\n",
    "            frame_root = match_dir / 'frame'\n",
    "            if not frame_root.exists():\n",
    "                continue\n",
    "            rally_medians = []\n",
    "            for rally_dir in tqdm(sorted(frame_root.iterdir()), desc=f'{split}/{match_dir.name}'):\n",
    "                if not rally_dir.is_dir():\n",
    "                    continue\n",
    "                median_file = rally_dir / 'median.npz'\n",
    "                if median_file.exists():\n",
    "                    rally_medians.append(np.load(str(median_file))['median'])\n",
    "                    continue\n",
    "                frames = sorted(rally_dir.glob('*.png'))\n",
    "                if not frames:\n",
    "                    continue\n",
    "                step = max(1, len(frames) // 50)\n",
    "                sampled = frames[::step][:50]\n",
    "                imgs = [cv2.imread(str(f))[..., ::-1] for f in sampled]\n",
    "                median = np.median(np.array(imgs), axis=0)\n",
    "                np.savez(str(median_file), median=median)\n",
    "                rally_medians.append(median)\n",
    "            # Match-level median\n",
    "            match_median = match_dir / 'median.npz'\n",
    "            if not match_median.exists() and rally_medians:\n",
    "                median = np.median(np.array(rally_medians), axis=0)\n",
    "                np.savez(str(match_median), median=median)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('median.npz files already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrained Weights\n",
    "\n",
    "Download the original TrackNetV3 badminton (shuttlecock) checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA\n",
      "From (redirected): https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA&confirm=t&uuid=5dda465a-d001-4285-8abf-2e29c67482c2\n",
      "To: /content/TrackNetV3_ckpts.zip\n",
      "100%|██████████| 132M/132M [00:00<00:00, 226MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  TrackNetV3_ckpts.zip\n",
      "   creating: /content/ckpts_tmp/ckpts/\n",
      "  inflating: /content/ckpts_tmp/ckpts/InpaintNet_best.pt  \n",
      "  inflating: /content/ckpts_tmp/ckpts/TrackNet_best.pt  \n",
      "Pretrained checkpoints:\n",
      "-rw-r--r-- 1 root root 6.0M Aug  8  2023 /content/tennis-tracknet/ckpts/InpaintNet_best.pt\n",
      "-rw-r--r-- 1 root root 130M Aug  8  2023 /content/tennis-tracknet/ckpts/TrackNet_best.pt\n",
      "\n",
      "Pretrained model: epoch 18, bg_mode='concat'\n",
      "Original training: seq_len=8, batch_size=10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "CKPT_DIR = '/content/tennis-tracknet/ckpts'\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(f'{CKPT_DIR}/TrackNet_best.pt'):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    \n",
    "    # Original TrackNetV3 checkpoints\n",
    "    # https://drive.google.com/file/d/1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA/view\n",
    "    gdown.download(\n",
    "        'https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA',\n",
    "        output='/content/TrackNetV3_ckpts.zip', quiet=False\n",
    "    )\n",
    "    !cd /content && unzip -o TrackNetV3_ckpts.zip -d /content/ckpts_tmp/\n",
    "    !rm /content/TrackNetV3_ckpts.zip\n",
    "    \n",
    "    # The zip contains a nested ckpts/ folder — flatten it\n",
    "    import glob, shutil\n",
    "    for pt_file in glob.glob('/content/ckpts_tmp/**/*.pt', recursive=True):\n",
    "        shutil.move(pt_file, CKPT_DIR)\n",
    "    !rm -rf /content/ckpts_tmp\n",
    "\n",
    "print('Pretrained checkpoints:')\n",
    "!ls -lh {CKPT_DIR}/*.pt\n",
    "\n",
    "# Verify checkpoint\n",
    "import torch\n",
    "ckpt = torch.load(f'{CKPT_DIR}/TrackNet_best.pt', map_location='cpu', weights_only=False)\n",
    "print(f\"\\nPretrained model: epoch {ckpt['epoch']}, bg_mode='{ckpt['param_dict']['bg_mode']}'\")\n",
    "print(f\"Original training: seq_len={ckpt['param_dict']['seq_len']}, batch_size={ckpt['param_dict']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Precompute Frames (one-time)\n",
    "\n",
    "Precomputes resized + background-subtracted frames as `.npy` files for fast data loading.\n",
    "Cached in Google Drive so this only runs once (~20 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Precompute frames (skip if already done)\n# Since data/ is symlinked to Drive, output goes to Drive automatically\n# Uses parallel workers + threaded I/O for ~3-4x speedup on Drive\nimport os, glob\n\nPRECOMPUTE_DIR = os.path.join(DATA_LOCAL, 'precomputed', 'subtract_concat_576x1024')\n\nexisting = glob.glob(os.path.join(PRECOMPUTE_DIR, '*.npy'))\nif len(existing) > 90:\n    print(f'Precomputed frames found: {len(existing)} files — skipping')\nelse:\n    print(f'Precomputing frames ({len(existing)} found, need ~95)...')\n    !python precompute_frames.py \\\n        --data_dir {DATA_LOCAL} \\\n        --bg_mode subtract_concat \\\n        --height 576 --width 1024 \\\n        --workers 4 --io_threads 8 \\\n        --splits train val"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint: epoch 3, best_val_acc=0.3011\n"
     ]
    }
   ],
   "source": [
    "# Upload local checkpoint to resume training on Colab\n",
    "# Run this cell, then select your TrackNet2x_cur.pt file from local machine\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "SAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink exps into repo\n",
    "!mkdir -p {DRIVE_DIR}/exps\n",
    "!rm -rf /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "# Upload checkpoint\n",
    "# print('Upload TrackNet2x_cur.pt (and optionally TrackNet2x_best.pt):')\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for name, data in uploaded.items():\n",
    "#     dest = os.path.join(SAVE_DIR, name)\n",
    "#     with open(dest, 'wb') as f:\n",
    "#         f.write(data)\n",
    "#     print(f'Saved {name} to {dest} ({len(data)/1e6:.1f} MB)')\n",
    "\n",
    "# Verify\n",
    "import torch\n",
    "ckpt = torch.load(os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt'), map_location='cpu', weights_only=False)\n",
    "print(f\"\\nCheckpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train\n",
    "\n",
    "**Option A:** Start fresh training from scratch (cell below).\n",
    "**Option B:** Resume from a local checkpoint — upload it first, then use the resume cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: tennis_2x_colab\n",
      "Batch size: 16 (GPU: 85 GB)\n",
      "Checkpoints: /content/drive/MyDrive/tennis-tracknet/exps/tennis_2x_colab\n",
      "Found checkpoint: epoch 3, best_val_acc=0.3011\n",
      "Use the RESUME cell below to continue training.\n"
     ]
    }
   ],
   "source": [
    "# Training config (used by both fresh start and resume)\n",
    "import os, torch\n",
    "\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "SAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink exps into repo so train.py can find them\n",
    "!mkdir -p {DRIVE_DIR}/exps\n",
    "!rm -rf /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "# Batch size: 16 for A100, 8 for T4/V100, 4 for P100\n",
    "mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "BATCH_SIZE = 16 if mem_gb >= 40 else (8 if mem_gb >= 15 else 4)\n",
    "\n",
    "print(f'Experiment: {EXP_NAME}')\n",
    "print(f'Batch size: {BATCH_SIZE} (GPU: {mem_gb:.0f} GB)')\n",
    "print(f'Checkpoints: {SAVE_DIR}')\n",
    "\n",
    "# Check for existing checkpoint\n",
    "ckpt_path = os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt')\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    print(f\"Found checkpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")\n",
    "    print('Use the RESUME cell below to continue training.')\n",
    "else:\n",
    "    print('No checkpoint found. Use the FRESH START cell below.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-15 02:04:21.732634: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-15 02:04:21.751246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771121061.774492    4225 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771121061.782074    4225 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771121061.801821    4225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771121061.801855    4225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771121061.801859    4225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771121061.801863    4225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-15 02:04:21.807163: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "TensorBoard: start with 'tensorboard --logdir exps/tennis_2x_colab/logs', view at http://localhost:6006/\n",
      "Load checkpoint from TrackNet2x_cur.pt...\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/tennis-tracknet/train.py\", line 255, in <module>\n",
      "    ckpt = torch.load(os.path.join(args.save_dir, f'{args.model_name}_cur.pt'), map_location='cpu')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
      "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "# Resume training (run Setup cells 1-5 first, then this)\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "\n",
    "# Re-link Drive dirs\n",
    "!rm -rf /content/tennis-tracknet/data /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/data /content/tennis-tracknet/data\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "!python train.py \\\n",
    "    --model_name TrackNet2x \\\n",
    "    --epochs 30 \\\n",
    "    --fp16 \\\n",
    "    --compile \\\n",
    "    --num_workers 4 \\\n",
    "    --save_dir exps/{EXP_NAME} \\\n",
    "    --resume_training \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUME — continue training from checkpoint (local or previous Colab run)\n",
    "!python train.py \\\n",
    "    --model_name TrackNet2x \\\n",
    "    --epochs 30 \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --fp16 \\\n",
    "    --compile \\\n",
    "    --num_workers 4 \\\n",
    "    --save_dir exps/{EXP_NAME} \\\n",
    "    --resume_training \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {DRIVE_DIR}/exps/{EXP_NAME}/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "\n",
    "!python test.py \\\n",
    "    --split test \\\n",
    "    --tracknet_file exps/{EXP_NAME}/TrackNet2x_best.pt \\\n",
    "    --save_dir exps/{EXP_NAME}/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy best model back to Drive for download\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "!cp exps/{EXP_NAME}/TrackNet2x_best.pt {DRIVE_DIR}/{EXP_NAME}_best.pt\n",
    "print(f'Best model saved to: {DRIVE_DIR}/{EXP_NAME}_best.pt')\n",
    "print('You can download it from Google Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}