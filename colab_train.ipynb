{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tennis TrackNet 2x — Colab Training\n",
    "\n",
    "Train **TrackNet2x** (576x1024 resolution) on the tennis dataset with GPU optimizations.\n",
    "\n",
    "**Optimizations enabled on Colab:**\n",
    "- Mixed precision (FP16) — ~2x speedup\n",
    "- torch.compile — ~1.3x speedup\n",
    "- Larger batch size (8-16 vs 2-4 locally)\n",
    "- Precomputed frames for fast data loading\n",
    "\n",
    "**Requirements:** GPU runtime (T4 or better), Google Drive mounted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-80GB, 81920 MiB\n",
      "PyTorch: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Memory: 85.1 GB\n",
      "Recommended batch_size: 8\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f'Memory: {mem_gb:.1f} GB')\n",
    "    # Recommend batch size based on GPU memory\n",
    "    # batch_size=16 OOMs on A100 with torch.compile; 8 is safe\n",
    "    if mem_gb >= 40:\n",
    "        print('Recommended batch_size: 8')\n",
    "    elif mem_gb >= 15:\n",
    "        print('Recommended batch_size: 4')\n",
    "    else:\n",
    "        print('Recommended batch_size: 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (for persistent storage across sessions)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Persistent storage dir\n",
    "DRIVE_DIR = '/content/drive/MyDrive/tennis-tracknet'\n",
    "!mkdir -p {DRIVE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/tennis-tracknet'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
      "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
      "remote: Total 91 (delta 50), reused 69 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (91/91), 88.37 KiB | 5.52 MiB/s, done.\n",
      "Resolving deltas: 100% (50/50), done.\n",
      "/content/tennis-tracknet\n"
     ]
    }
   ],
   "source": [
    "# Clone private repo (will prompt for GitHub auth)\n",
    "import os\n",
    "if not os.path.exists('/content/tennis-tracknet'):\n",
    "    !git clone https://github.com/smyng/tennis-tracknet.git /content/tennis-tracknet\n",
    "else:\n",
    "    !cd /content/tennis-tracknet && git pull\n",
    "\n",
    "os.chdir('/content/tennis-tracknet')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q parse tqdm tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Downloads the TrackNet v1 tennis dataset and converts it to TrackNetV3 format.\n",
    "Converted data is cached in Google Drive so you only do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted dataset found in Drive, symlinking...\n",
      "match1\tmatch2\tmatch3\tmatch4\tmatch5\tmatch6\tmatch7\tmatch8\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DRIVE = f'{DRIVE_DIR}/data'\n",
    "DATA_LOCAL = '/content/tennis-tracknet/data'\n",
    "\n",
    "# Check if converted data already exists in Drive\n",
    "if os.path.exists(f'{DATA_DRIVE}/train/match1'):\n",
    "    print('Converted dataset found in Drive, symlinking...')\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    !ls {DATA_LOCAL}/train/\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('No converted dataset in Drive. Will download and convert.')\n",
    "    print('This takes ~15-20 min the first time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download raw tennis dataset (skip if data already linked above)\n",
    "import os\n",
    "if not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    RAW_DIR = '/content/raw-tennis-dataset'\n",
    "    !mkdir -p {RAW_DIR}\n",
    "    \n",
    "    # Download from the TrackNet v1 dataset Google Drive\n",
    "    # Folder: https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut\n",
    "    import gdown\n",
    "    gdown.download_folder(\n",
    "        'https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut',\n",
    "        output=RAW_DIR, quiet=False\n",
    "    )\n",
    "    \n",
    "    # The dataset is inside Dataset.zip — unzip it\n",
    "    import zipfile\n",
    "    zip_path = os.path.join(RAW_DIR, 'Dataset.zip')\n",
    "    if os.path.exists(zip_path):\n",
    "        print('Extracting Dataset.zip...')\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(RAW_DIR)\n",
    "        os.remove(zip_path)\n",
    "    \n",
    "    # Find the directory containing game1/, game2/, etc.\n",
    "    DATASET_DIR = RAW_DIR\n",
    "    for candidate in [os.path.join(RAW_DIR, 'Dataset'), RAW_DIR]:\n",
    "        if os.path.exists(os.path.join(candidate, 'game1')):\n",
    "            DATASET_DIR = candidate\n",
    "            break\n",
    "    \n",
    "    print(f'Dataset directory: {DATASET_DIR}')\n",
    "    !ls {DATASET_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset and save to Drive for persistence\n",
    "import os\n",
    "if not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n",
    "    # Find the dataset directory (set by previous cell, or detect it)\n",
    "    if 'DATASET_DIR' not in dir():\n",
    "        RAW_DIR = '/content/raw-tennis-dataset'\n",
    "        DATASET_DIR = os.path.join(RAW_DIR, 'Dataset') if os.path.exists(os.path.join(RAW_DIR, 'Dataset', 'game1')) else RAW_DIR\n",
    "\n",
    "    !python scripts/convert_tennis_dataset.py \\\n",
    "        --input {DATASET_DIR} \\\n",
    "        --output {DATA_DRIVE} \\\n",
    "        --test-games 9 10 \\\n",
    "        --verbose\n",
    "    \n",
    "    # Symlink Drive data into repo\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    \n",
    "    print('Conversion complete.')\n",
    "    !ls {DATA_LOCAL}/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median.npz files already exist.\n"
     ]
    }
   ],
   "source": [
    "# Generate median.npz files (required by dataset.py for background subtraction)\n",
    "# Saved to Drive, so this only runs once\n",
    "import os, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = DATA_DRIVE\n",
    "needs_generation = False\n",
    "\n",
    "# Check if median files already exist\n",
    "for split in ['train', 'test']:\n",
    "    split_dir = Path(data_dir) / split\n",
    "    if not split_dir.exists():\n",
    "        continue\n",
    "    for match_dir in split_dir.iterdir():\n",
    "        frame_root = match_dir / 'frame'\n",
    "        if not frame_root.exists():\n",
    "            continue\n",
    "        for rally_dir in frame_root.iterdir():\n",
    "            if rally_dir.is_dir() and not (rally_dir / 'median.npz').exists():\n",
    "                needs_generation = True\n",
    "                break\n",
    "        if needs_generation:\n",
    "            break\n",
    "    if needs_generation:\n",
    "        break\n",
    "\n",
    "if needs_generation:\n",
    "    print('Generating median.npz files (one-time)...')\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = Path(data_dir) / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        for match_dir in sorted(split_dir.iterdir()):\n",
    "            if not match_dir.is_dir():\n",
    "                continue\n",
    "            frame_root = match_dir / 'frame'\n",
    "            if not frame_root.exists():\n",
    "                continue\n",
    "            rally_medians = []\n",
    "            for rally_dir in tqdm(sorted(frame_root.iterdir()), desc=f'{split}/{match_dir.name}'):\n",
    "                if not rally_dir.is_dir():\n",
    "                    continue\n",
    "                median_file = rally_dir / 'median.npz'\n",
    "                if median_file.exists():\n",
    "                    rally_medians.append(np.load(str(median_file))['median'])\n",
    "                    continue\n",
    "                frames = sorted(rally_dir.glob('*.png'))\n",
    "                if not frames:\n",
    "                    continue\n",
    "                step = max(1, len(frames) // 50)\n",
    "                sampled = frames[::step][:50]\n",
    "                imgs = [cv2.imread(str(f))[..., ::-1] for f in sampled]\n",
    "                median = np.median(np.array(imgs), axis=0)\n",
    "                np.savez(str(median_file), median=median)\n",
    "                rally_medians.append(median)\n",
    "            # Match-level median\n",
    "            match_median = match_dir / 'median.npz'\n",
    "            if not match_median.exists() and rally_medians:\n",
    "                median = np.median(np.array(rally_medians), axis=0)\n",
    "                np.savez(str(match_median), median=median)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('median.npz files already exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrained Weights\n",
    "\n",
    "Download the original TrackNetV3 badminton (shuttlecock) checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA\n",
      "From (redirected): https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA&confirm=t&uuid=f55a0403-da02-44c2-b2d6-0bea5276ce44\n",
      "To: /content/TrackNetV3_ckpts.zip\n",
      "100%|██████████| 132M/132M [00:01<00:00, 102MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  TrackNetV3_ckpts.zip\n",
      "   creating: /content/ckpts_tmp/ckpts/\n",
      "  inflating: /content/ckpts_tmp/ckpts/InpaintNet_best.pt  \n",
      "  inflating: /content/ckpts_tmp/ckpts/TrackNet_best.pt  \n",
      "Pretrained checkpoints:\n",
      "-rw-r--r-- 1 root root 6.0M Aug  8  2023 /content/tennis-tracknet/ckpts/InpaintNet_best.pt\n",
      "-rw-r--r-- 1 root root 130M Aug  8  2023 /content/tennis-tracknet/ckpts/TrackNet_best.pt\n",
      "\n",
      "Pretrained model: epoch 18, bg_mode='concat'\n",
      "Original training: seq_len=8, batch_size=10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "CKPT_DIR = '/content/tennis-tracknet/ckpts'\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(f'{CKPT_DIR}/TrackNet_best.pt'):\n",
    "    !pip install -q gdown\n",
    "    import gdown\n",
    "    \n",
    "    # Original TrackNetV3 checkpoints\n",
    "    # https://drive.google.com/file/d/1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA/view\n",
    "    gdown.download(\n",
    "        'https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA',\n",
    "        output='/content/TrackNetV3_ckpts.zip', quiet=False\n",
    "    )\n",
    "    !cd /content && unzip -o TrackNetV3_ckpts.zip -d /content/ckpts_tmp/\n",
    "    !rm /content/TrackNetV3_ckpts.zip\n",
    "    \n",
    "    # The zip contains a nested ckpts/ folder — flatten it\n",
    "    import glob, shutil\n",
    "    for pt_file in glob.glob('/content/ckpts_tmp/**/*.pt', recursive=True):\n",
    "        shutil.move(pt_file, CKPT_DIR)\n",
    "    !rm -rf /content/ckpts_tmp\n",
    "\n",
    "print('Pretrained checkpoints:')\n",
    "!ls -lh {CKPT_DIR}/*.pt\n",
    "\n",
    "# Verify checkpoint\n",
    "import torch\n",
    "ckpt = torch.load(f'{CKPT_DIR}/TrackNet_best.pt', map_location='cpu', weights_only=False)\n",
    "print(f\"\\nPretrained model: epoch {ckpt['epoch']}, bg_mode='{ckpt['param_dict']['bg_mode']}'\")\n",
    "print(f\"Original training: seq_len={ckpt['param_dict']['seq_len']}, batch_size={ckpt['param_dict']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Precompute Frames (one-time)\n",
    "\n",
    "Precomputes resized + background-subtracted frames as `.npy` files for fast data loading.\n",
    "Cached in Google Drive so this only runs once (~20 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed frames found: 95 files — skipping\n"
     ]
    }
   ],
   "source": [
    "# Precompute frames (skip if already done)\n",
    "# Since data/ is symlinked to Drive, output goes to Drive automatically\n",
    "# Uses parallel workers + threaded I/O for ~3-4x speedup on Drive\n",
    "import os, glob\n",
    "\n",
    "PRECOMPUTE_DIR = os.path.join(DATA_LOCAL, 'precomputed', 'subtract_concat_576x1024')\n",
    "\n",
    "existing = glob.glob(os.path.join(PRECOMPUTE_DIR, '*.npy'))\n",
    "if len(existing) > 90:\n",
    "    print(f'Precomputed frames found: {len(existing)} files — skipping')\n",
    "else:\n",
    "    print(f'Precomputing frames ({len(existing)} found, need ~95)...')\n",
    "    !python precompute_frames.py \\\n",
    "        --data_dir {DATA_LOCAL} \\\n",
    "        --bg_mode subtract_concat \\\n",
    "        --height 576 --width 1024 \\\n",
    "        --workers 4 --io_threads 8 \\\n",
    "        --splits train val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying precomputed frames from Drive to local SSD...\n",
      "Done in 13.2 min\n",
      "44G\t/content/precomputed_local\n",
      "PRECOMPUTED_DIR=/content/precomputed_local\n"
     ]
    }
   ],
   "source": [
    "# Copy precomputed frames to local SSD for ~10x faster data loading\n",
    "# Google Drive FUSE is slow for large file reads — local SSD eliminates the I/O bottleneck\n",
    "# This takes ~5-10 min but saves significant time per epoch\n",
    "import os, time\n",
    "\n",
    "PRECOMPUTE_LOCAL = '/content/precomputed_local'\n",
    "\n",
    "if not os.path.exists(PRECOMPUTE_LOCAL):\n",
    "    print('Copying precomputed frames from Drive to local SSD...')\n",
    "    start = time.time()\n",
    "    !cp -r {DATA_DRIVE}/precomputed {PRECOMPUTE_LOCAL}\n",
    "    elapsed = time.time() - start\n",
    "    print(f'Done in {elapsed/60:.1f} min')\n",
    "    !du -sh {PRECOMPUTE_LOCAL}\n",
    "else:\n",
    "    print('Local precomputed copy already exists')\n",
    "    !du -sh {PRECOMPUTE_LOCAL}\n",
    "\n",
    "# Set env var so dataset.py reads precomputed from local SSD instead of Drive\n",
    "os.environ['PRECOMPUTED_DIR'] = PRECOMPUTE_LOCAL\n",
    "print(f'PRECOMPUTED_DIR={PRECOMPUTE_LOCAL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint: epoch 5, best_val_acc=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Upload local checkpoint to resume training on Colab\n",
    "# Run this cell, then select your TrackNet2x_cur.pt file from local machine\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "SAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink exps into repo\n",
    "!mkdir -p {DRIVE_DIR}/exps\n",
    "!rm -rf /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "# Upload checkpoint\n",
    "# print('Upload TrackNet2x_cur.pt (and optionally TrackNet2x_best.pt):')\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for name, data in uploaded.items():\n",
    "#     dest = os.path.join(SAVE_DIR, name)\n",
    "#     with open(dest, 'wb') as f:\n",
    "#         f.write(data)\n",
    "#     print(f'Saved {name} to {dest} ({len(data)/1e6:.1f} MB)')\n",
    "\n",
    "# Verify\n",
    "import torch\n",
    "ckpt = torch.load(os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt'), map_location='cpu', weights_only=False)\n",
    "print(f\"\\nCheckpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train\n",
    "\n",
    "**Option A:** Start fresh training from scratch (cell below).\n",
    "**Option B:** Resume from a local checkpoint — upload it first, then use the resume cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: tennis_2x_colab\n",
      "Batch size: 8 (GPU: 85 GB)\n",
      "Checkpoints: /content/drive/MyDrive/tennis-tracknet/exps/tennis_2x_colab\n",
      "Found checkpoint: epoch 5, best_val_acc=1.0000\n",
      "Use the RESUME cell below to continue training.\n"
     ]
    }
   ],
   "source": [
    "# Training config (used by both fresh start and resume)\n",
    "import os, torch, subprocess\n",
    "\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "SAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Symlink exps into repo so train.py can find them\n",
    "!mkdir -p {DRIVE_DIR}/exps\n",
    "!rm -rf /content/tennis-tracknet/exps\n",
    "!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n",
    "\n",
    "# Kill any zombie GPU processes from interrupted training runs\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-compute-apps=pid', '--format=csv,noheader'],\n",
    "        capture_output=True, text=True)\n",
    "    my_pid = str(os.getpid())\n",
    "    for pid in result.stdout.strip().split('\\n'):\n",
    "        pid = pid.strip()\n",
    "        if pid and pid != my_pid:\n",
    "            subprocess.run(['kill', '-9', pid])\n",
    "            print(f'Killed zombie GPU process {pid}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Batch size: 8 for A100 (16 OOMs with torch.compile), 4 for T4/V100\n",
    "mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "BATCH_SIZE = 8 if mem_gb >= 40 else (4 if mem_gb >= 15 else 2)\n",
    "\n",
    "print(f'Experiment: {EXP_NAME}')\n",
    "print(f'Batch size: {BATCH_SIZE} (GPU: {mem_gb:.0f} GB)')\n",
    "print(f'Checkpoints: {SAVE_DIR}')\n",
    "\n",
    "# Check for existing checkpoint\n",
    "ckpt_path = os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt')\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "    print(f\"Found checkpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")\n",
    "    print('Use the RESUME cell below to continue training.')\n",
    "else:\n",
    "    print('No checkpoint found. Use the FRESH START cell below.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# FRESH START — train from scratch (no checkpoint needed)\n# Delete any corrupted checkpoints first\nimport os\nfor f in ['TrackNet2x_cur.pt', 'TrackNet2x_best.pt']:\n    p = os.path.join(SAVE_DIR, f)\n    if os.path.exists(p):\n        os.remove(p)\n        print(f'Removed {f}')\n\n!python train.py \\\n    --model_name TrackNet2x \\\n    --epochs 30 \\\n    --batch_size {BATCH_SIZE} \\\n    --bg_mode subtract_concat \\\n    --height 576 --width 1024 \\\n    --fp16 \\\n    --compile \\\n    --num_workers 2 \\\n    --frame_alpha 0.5 \\\n    --save_dir exps/{EXP_NAME} \\\n    --verbose"
  },
  {
   "cell_type": "code",
   "source": "# RESUME — continue training from checkpoint\n# Only use this if you have a valid checkpoint in SAVE_DIR\n!python train.py \\\n    --model_name TrackNet2x \\\n    --epochs 30 \\\n    --batch_size {BATCH_SIZE} \\\n    --bg_mode subtract_concat \\\n    --height 576 --width 1024 \\\n    --fp16 \\\n    --compile \\\n    --num_workers 2 \\\n    --frame_alpha 0.5 \\\n    --save_dir exps/{EXP_NAME} \\\n    --resume_training \\\n    --verbose",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {DRIVE_DIR}/exps/{EXP_NAME}/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/tennis-tracknet/test.py\", line 938, in <module>\n",
      "    tracknet_ckpt = torch.load(args.tracknet_file, weights_only=False)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1530, in load\n",
      "    return _load(\n",
      "           ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2122, in _load\n",
      "    result = unpickler.load()\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2086, in persistent_load\n",
      "    typed_storage = load_tensor(\n",
      "                    ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 2052, in load_tensor\n",
      "    wrap_storage = restore_location(storage, location)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 698, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 564, in _mps_deserialize\n",
      "    return obj.mps()\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/storage.py\", line 272, in mps\n",
      "    return torch.UntypedStorage(self.size(), device=\"mps\").copy_(self, False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: torch.UntypedStorage(): Storage device not recognized: mps\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on test set\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "\n",
    "!python test.py \\\n",
    "    --split test \\\n",
    "    --tracknet_file exps/{EXP_NAME}/TrackNet2x_best.pt \\\n",
    "    --save_dir exps/{EXP_NAME}/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to: /content/drive/MyDrive/tennis-tracknet/tennis_2x_colab_best.pt\n",
      "You can download it from Google Drive.\n"
     ]
    }
   ],
   "source": [
    "# Copy best model back to Drive for download\n",
    "EXP_NAME = 'tennis_2x_colab'\n",
    "!cp exps/{EXP_NAME}/TrackNet2x_best.pt {DRIVE_DIR}/{EXP_NAME}_best.pt\n",
    "print(f'Best model saved to: {DRIVE_DIR}/{EXP_NAME}_best.pt')\n",
    "print('You can download it from Google Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}