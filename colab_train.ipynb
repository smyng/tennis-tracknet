{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tennis TrackNet 2x — Colab Training\n\nTrain **TrackNet2x** (576x1024 resolution) on the tennis dataset with GPU optimizations.\n\n**Optimizations enabled on Colab:**\n- Mixed precision (FP16) — ~2x speedup\n- torch.compile — ~1.3x speedup\n- Larger batch size (8-16 vs 2-4 locally)\n- Precomputed frames for fast data loading\n\n**Requirements:** GPU runtime (T4 or better), Google Drive mounted"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU\n!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n\nimport torch\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    mem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n    print(f'Memory: {mem_gb:.1f} GB')\n    # Recommend batch size based on GPU memory\n    if mem_gb >= 40:\n        print('Recommended batch_size: 16')\n    elif mem_gb >= 15:\n        print('Recommended batch_size: 8')\n    else:\n        print('Recommended batch_size: 4')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for persistent storage across sessions)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Persistent storage dir\n",
    "DRIVE_DIR = '/content/drive/MyDrive/tennis-tracknet'\n",
    "!mkdir -p {DRIVE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone private repo (will prompt for GitHub auth)\n",
    "import os\n",
    "if not os.path.exists('/content/tennis-tracknet'):\n",
    "    !git clone https://github.com/smyng/tennis-tracknet.git /content/tennis-tracknet\n",
    "else:\n",
    "    !cd /content/tennis-tracknet && git pull\n",
    "\n",
    "os.chdir('/content/tennis-tracknet')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q parse tqdm tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "Downloads the TrackNet v1 tennis dataset and converts it to TrackNetV3 format.\n",
    "Converted data is cached in Google Drive so you only do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DRIVE = f'{DRIVE_DIR}/data'\n",
    "DATA_LOCAL = '/content/tennis-tracknet/data'\n",
    "\n",
    "# Check if converted data already exists in Drive\n",
    "if os.path.exists(f'{DATA_DRIVE}/train/match1'):\n",
    "    print('Converted dataset found in Drive, symlinking...')\n",
    "    !rm -rf {DATA_LOCAL}\n",
    "    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n",
    "    !ls {DATA_LOCAL}/train/\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('No converted dataset in Drive. Will download and convert.')\n",
    "    print('This takes ~15-20 min the first time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download raw tennis dataset (skip if data already linked above)\nimport os\nif not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n    !pip install -q gdown\n    \n    RAW_DIR = '/content/raw-tennis-dataset'\n    !mkdir -p {RAW_DIR}\n    \n    # Download from the TrackNet v1 dataset Google Drive\n    # Folder: https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut\n    import gdown\n    gdown.download_folder(\n        'https://drive.google.com/drive/folders/11r0RUaQHX7I3ANkaYG4jOxXK1OYo01Ut',\n        output=RAW_DIR, quiet=False\n    )\n    \n    # The dataset is inside Dataset.zip — unzip it\n    import zipfile\n    zip_path = os.path.join(RAW_DIR, 'Dataset.zip')\n    if os.path.exists(zip_path):\n        print('Extracting Dataset.zip...')\n        with zipfile.ZipFile(zip_path, 'r') as z:\n            z.extractall(RAW_DIR)\n        os.remove(zip_path)\n    \n    # Find the directory containing game1/, game2/, etc.\n    DATASET_DIR = RAW_DIR\n    for candidate in [os.path.join(RAW_DIR, 'Dataset'), RAW_DIR]:\n        if os.path.exists(os.path.join(candidate, 'game1')):\n            DATASET_DIR = candidate\n            break\n    \n    print(f'Dataset directory: {DATASET_DIR}')\n    !ls {DATASET_DIR}/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert dataset and save to Drive for persistence\nimport os\nif not os.path.exists(f'{DATA_LOCAL}/train/match1'):\n    # Find the dataset directory (set by previous cell, or detect it)\n    if 'DATASET_DIR' not in dir():\n        RAW_DIR = '/content/raw-tennis-dataset'\n        DATASET_DIR = os.path.join(RAW_DIR, 'Dataset') if os.path.exists(os.path.join(RAW_DIR, 'Dataset', 'game1')) else RAW_DIR\n\n    !python scripts/convert_tennis_dataset.py \\\n        --input {DATASET_DIR} \\\n        --output {DATA_DRIVE} \\\n        --test-games 9 10 \\\n        --verbose\n    \n    # Symlink Drive data into repo\n    !rm -rf {DATA_LOCAL}\n    !ln -s {DATA_DRIVE} {DATA_LOCAL}\n    \n    print('Conversion complete.')\n    !ls {DATA_LOCAL}/train/"
  },
  {
   "cell_type": "code",
   "source": "# Generate median.npz files (required by dataset.py for background subtraction)\n# Saved to Drive, so this only runs once\nimport os, cv2, numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndata_dir = DATA_DRIVE\nneeds_generation = False\n\n# Check if median files already exist\nfor split in ['train', 'test']:\n    split_dir = Path(data_dir) / split\n    if not split_dir.exists():\n        continue\n    for match_dir in split_dir.iterdir():\n        frame_root = match_dir / 'frame'\n        if not frame_root.exists():\n            continue\n        for rally_dir in frame_root.iterdir():\n            if rally_dir.is_dir() and not (rally_dir / 'median.npz').exists():\n                needs_generation = True\n                break\n        if needs_generation:\n            break\n    if needs_generation:\n        break\n\nif needs_generation:\n    print('Generating median.npz files (one-time)...')\n    for split in ['train', 'test']:\n        split_dir = Path(data_dir) / split\n        if not split_dir.exists():\n            continue\n        for match_dir in sorted(split_dir.iterdir()):\n            if not match_dir.is_dir():\n                continue\n            frame_root = match_dir / 'frame'\n            if not frame_root.exists():\n                continue\n            rally_medians = []\n            for rally_dir in tqdm(sorted(frame_root.iterdir()), desc=f'{split}/{match_dir.name}'):\n                if not rally_dir.is_dir():\n                    continue\n                median_file = rally_dir / 'median.npz'\n                if median_file.exists():\n                    rally_medians.append(np.load(str(median_file))['median'])\n                    continue\n                frames = sorted(rally_dir.glob('*.png'))\n                if not frames:\n                    continue\n                step = max(1, len(frames) // 50)\n                sampled = frames[::step][:50]\n                imgs = [cv2.imread(str(f))[..., ::-1] for f in sampled]\n                median = np.median(np.array(imgs), axis=0)\n                np.savez(str(median_file), median=median)\n                rally_medians.append(median)\n            # Match-level median\n            match_median = match_dir / 'median.npz'\n            if not match_median.exists() and rally_medians:\n                median = np.median(np.array(rally_medians), axis=0)\n                np.savez(str(match_median), median=median)\n    print('Done.')\nelse:\n    print('median.npz files already exist.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pretrained Weights\n",
    "\n",
    "Download the original TrackNetV3 badminton (shuttlecock) checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nCKPT_DIR = '/content/tennis-tracknet/ckpts'\nos.makedirs(CKPT_DIR, exist_ok=True)\n\nif not os.path.exists(f'{CKPT_DIR}/TrackNet_best.pt'):\n    !pip install -q gdown\n    import gdown\n    \n    # Original TrackNetV3 checkpoints\n    # https://drive.google.com/file/d/1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA/view\n    gdown.download(\n        'https://drive.google.com/uc?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA',\n        output='/content/TrackNetV3_ckpts.zip', quiet=False\n    )\n    !cd /content && unzip -o TrackNetV3_ckpts.zip -d /content/ckpts_tmp/\n    !rm /content/TrackNetV3_ckpts.zip\n    \n    # The zip contains a nested ckpts/ folder — flatten it\n    import glob, shutil\n    for pt_file in glob.glob('/content/ckpts_tmp/**/*.pt', recursive=True):\n        shutil.move(pt_file, CKPT_DIR)\n    !rm -rf /content/ckpts_tmp\n\nprint('Pretrained checkpoints:')\n!ls -lh {CKPT_DIR}/*.pt\n\n# Verify checkpoint\nimport torch\nckpt = torch.load(f'{CKPT_DIR}/TrackNet_best.pt', map_location='cpu', weights_only=False)\nprint(f\"\\nPretrained model: epoch {ckpt['epoch']}, bg_mode='{ckpt['param_dict']['bg_mode']}'\")\nprint(f\"Original training: seq_len={ckpt['param_dict']['seq_len']}, batch_size={ckpt['param_dict']['batch_size']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Precompute Frames (one-time)\n\nPrecomputes resized + background-subtracted frames as `.npy` files for fast data loading.\nCached in Google Drive so this only runs once (~20 min)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Precompute frames (skip if already done)\n# Since data/ is symlinked to Drive, output goes to Drive automatically\nimport os, glob\n\nPRECOMPUTE_DIR = os.path.join(DATA_LOCAL, 'precomputed', 'subtract_concat_576x1024')\n\nexisting = glob.glob(os.path.join(PRECOMPUTE_DIR, '*.npy'))\nif len(existing) > 90:\n    print(f'Precomputed frames found: {len(existing)} files — skipping')\nelse:\n    print(f'Precomputing frames ({len(existing)} found, need ~95)...')\n    !python precompute_frames.py \\\n        --data_dir {DATA_LOCAL} \\\n        --bg_mode subtract_concat \\\n        --height 576 --width 1024 \\\n        --splits train val"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload local checkpoint to resume training on Colab\n# Run this cell, then select your TrackNet2x_cur.pt file from local machine\nimport os\nfrom google.colab import files\n\nEXP_NAME = 'tennis_2x_colab'\nSAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Symlink exps into repo\n!mkdir -p {DRIVE_DIR}/exps\n!rm -rf /content/tennis-tracknet/exps\n!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n\n# Upload checkpoint\nprint('Upload TrackNet2x_cur.pt (and optionally TrackNet2x_best.pt):')\nuploaded = files.upload()\n\nfor name, data in uploaded.items():\n    dest = os.path.join(SAVE_DIR, name)\n    with open(dest, 'wb') as f:\n        f.write(data)\n    print(f'Saved {name} to {dest} ({len(data)/1e6:.1f} MB)')\n\n# Verify\nimport torch\nckpt = torch.load(os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt'), map_location='cpu', weights_only=False)\nprint(f\"\\nCheckpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Train\n\n**Option A:** Start fresh training from scratch (cell below).\n**Option B:** Resume from a local checkpoint — upload it first, then use the resume cell.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Training config (used by both fresh start and resume)\nimport os, torch\n\nEXP_NAME = 'tennis_2x_colab'\nSAVE_DIR = f'{DRIVE_DIR}/exps/{EXP_NAME}'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Symlink exps into repo so train.py can find them\n!mkdir -p {DRIVE_DIR}/exps\n!rm -rf /content/tennis-tracknet/exps\n!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n\n# Batch size: 16 for A100, 8 for T4/V100, 4 for P100\nmem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\nBATCH_SIZE = 16 if mem_gb >= 40 else (8 if mem_gb >= 15 else 4)\n\nprint(f'Experiment: {EXP_NAME}')\nprint(f'Batch size: {BATCH_SIZE} (GPU: {mem_gb:.0f} GB)')\nprint(f'Checkpoints: {SAVE_DIR}')\n\n# Check for existing checkpoint\nckpt_path = os.path.join(SAVE_DIR, 'TrackNet2x_cur.pt')\nif os.path.exists(ckpt_path):\n    ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n    print(f\"Found checkpoint: epoch {ckpt['epoch']+1}, best_val_acc={ckpt['max_val_acc']:.4f}\")\n    print('Use the RESUME cell below to continue training.')\nelse:\n    print('No checkpoint found. Use the FRESH START cell below.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Resume training (run Setup cells 1-5 first, then this)\nEXP_NAME = 'tennis_2x_colab'\n\n# Re-link Drive dirs\n!rm -rf /content/tennis-tracknet/data /content/tennis-tracknet/exps\n!ln -s {DRIVE_DIR}/data /content/tennis-tracknet/data\n!ln -s {DRIVE_DIR}/exps /content/tennis-tracknet/exps\n\n!python train.py \\\n    --model_name TrackNet2x \\\n    --epochs 30 \\\n    --fp16 \\\n    --compile \\\n    --num_workers 4 \\\n    --save_dir exps/{EXP_NAME} \\\n    --resume_training \\\n    --verbose",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# RESUME — continue training from checkpoint (local or previous Colab run)\n!python train.py \\\n    --model_name TrackNet2x \\\n    --epochs 30 \\\n    --batch_size {BATCH_SIZE} \\\n    --fp16 \\\n    --compile \\\n    --num_workers 4 \\\n    --save_dir exps/{EXP_NAME} \\\n    --resume_training \\\n    --verbose",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TensorBoard\nEXP_NAME = 'tennis_2x_colab'\n%load_ext tensorboard\n%tensorboard --logdir {DRIVE_DIR}/exps/{EXP_NAME}/logs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate best model on test set\nEXP_NAME = 'tennis_2x_colab'\n\n!python test.py \\\n    --split test \\\n    --tracknet_file exps/{EXP_NAME}/TrackNet2x_best.pt \\\n    --save_dir exps/{EXP_NAME}/eval"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Copy best model back to Drive for download\nEXP_NAME = 'tennis_2x_colab'\n!cp exps/{EXP_NAME}/TrackNet2x_best.pt {DRIVE_DIR}/{EXP_NAME}_best.pt\nprint(f'Best model saved to: {DRIVE_DIR}/{EXP_NAME}_best.pt')\nprint('You can download it from Google Drive.')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}